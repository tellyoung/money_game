{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH2idWb2Q9LR"
      },
      "source": [
        "## Beat the Market: An Effective Intraday Momentum Strategy for the S&P500 ETF (SPY)\n",
        "\n",
        "This code allows you to replicate the results presented in the paper \"Beat the Market: An Effective Intraday Momentum Strategy for the S&P500 ETF (SPY)\" published by ConcretumGroup founder, **Carlo Zarattini**.\n",
        "\n",
        "Co-authored by **Mohamed Gabriel**.\n",
        "\n",
        "For detailed explanations, please refer to the full blog post.\n",
        "\n",
        "You can reach the authors of this code by email at [info@concretumgroup.com](mailto:info@concretumgroup.com).\n",
        "\n",
        "More information can be found at [www.concretumgroup.com](http://www.concretumgroup.com).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# 0. Needed Functions & Libraries"
      ],
      "metadata": {
        "id": "nAhtgZAuRFwr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9ZEK01SQ9LV"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from   datetime import datetime\n",
        "import numpy as np\n",
        "import pytz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from   matplotlib.ticker import FuncFormatter\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su1wTepkQ9LW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the API key and base URL\n",
        "API_KEY  = 'API_KEY'\n",
        "BASE_URL = 'https://api.polygon.io'\n",
        "\n",
        "# Define the rate limit enforcement based on the API tier\n",
        "ENFORCE_RATE_LIMIT = True\n",
        "\n",
        "def fetch_polygon_data(ticker, start_date, end_date, period, enforce_rate_limit=ENFORCE_RATE_LIMIT):\n",
        "    \"\"\"Fetch stock data from Polygon.io based on the given period (minute or day).\n",
        "       enforce_rate_limit: Set to True to enforce rate limits (suitable for free tiers), False for paid tiers with minimal or no rate limits.\n",
        "    \"\"\"\n",
        "    multiplier = '1'\n",
        "    timespan = period\n",
        "    limit = '50000'  # Maximum entries per request\n",
        "    eastern = pytz.timezone('America/New_York')  # Eastern Time Zone\n",
        "\n",
        "    url = f'{BASE_URL}/v2/aggs/ticker/{ticker}/range/{multiplier}/{timespan}/{start_date}/{end_date}?adjusted=false&sort=asc&limit={limit}&apiKey={API_KEY}'\n",
        "\n",
        "    data_list = []\n",
        "    request_count = 0\n",
        "    first_request_time = None\n",
        "\n",
        "    while True:\n",
        "        if enforce_rate_limit and request_count == 5:\n",
        "            elapsed_time = time.time() - first_request_time\n",
        "            if elapsed_time < 60:\n",
        "                wait_time = 60 - elapsed_time\n",
        "                print(f\"API rate limit reached. Waiting {wait_time:.2f} seconds before next request.\")\n",
        "                time.sleep(wait_time)\n",
        "            request_count = 0\n",
        "            first_request_time = time.time()  # Reset the timer after the wait\n",
        "\n",
        "        if first_request_time is None and enforce_rate_limit:\n",
        "            first_request_time = time.time()\n",
        "\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            error_message = response.json().get('error', 'No specific error message provided')\n",
        "            print(f\"Error fetching data: {error_message}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "        request_count += 1\n",
        "\n",
        "        results_count = len(data.get('results', []))\n",
        "        print(f\"Fetched {results_count} entries from API.\")\n",
        "\n",
        "        if 'results' in data:\n",
        "            for entry in data['results']:\n",
        "                utc_time = datetime.fromtimestamp(entry['t'] / 1000, pytz.utc)\n",
        "                eastern_time = utc_time.astimezone(eastern)\n",
        "\n",
        "                data_entry = {\n",
        "                    'volume': entry['v'],\n",
        "                    'open': entry['o'],\n",
        "                    'high': entry['h'],\n",
        "                    'low': entry['l'],\n",
        "                    'close': entry['c'],\n",
        "                    'caldt': eastern_time.replace(tzinfo=None)\n",
        "                }\n",
        "\n",
        "                if period == 'minute':\n",
        "                    if eastern_time.time() >= datetime.strptime('09:30', '%H:%M').time() and eastern_time.time() <= datetime.strptime('15:59', '%H:%M').time():\n",
        "                        data_list.append(data_entry)\n",
        "                else:\n",
        "                    data_list.append(data_entry)\n",
        "\n",
        "        if 'next_url' in data and data['next_url']:\n",
        "            url = data['next_url'] + '&apiKey=' + API_KEY\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(\"Data fetching complete.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def fetch_polygon_dividends(ticker):\n",
        "    \"\"\" Fetches dividend data from Polygon.io for a specified stock ticker. \"\"\"\n",
        "    url = f'{BASE_URL}/v3/reference/dividends?ticker={ticker}&limit=1000&apiKey={API_KEY}'\n",
        "\n",
        "    dividends_list = []\n",
        "    while True:\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "        if 'results' in data:\n",
        "            for entry in data['results']:\n",
        "                dividends_list.append({\n",
        "                    'caldt': datetime.strptime(entry['ex_dividend_date'], '%Y-%m-%d'),\n",
        "                    'dividend': entry['cash_amount']\n",
        "                })\n",
        "\n",
        "        if 'next_url' in data and data['next_url']:\n",
        "            url = data['next_url'] + '&apiKey=' + API_KEY\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(dividends_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRFH5hCGQ9LX"
      },
      "source": [
        "# 1. Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDujFFq3Q9LX"
      },
      "outputs": [],
      "source": [
        "ticker      = 'SPY'\n",
        "from_date   = '2022-05-09'\n",
        "until_date  = '2024-04-22'\n",
        "\n",
        "spy_intra_data = fetch_polygon_data(ticker, from_date, until_date, 'minute')\n",
        "spy_daily_data = fetch_polygon_data(ticker, from_date, until_date, 'day')\n",
        "dividends      = fetch_polygon_dividends(ticker)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EooL9TqQ9LX"
      },
      "source": [
        "# 2. Add Key Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLeIKrJAQ9LY"
      },
      "outputs": [],
      "source": [
        "# Load the intraday data into a DataFrame and set the datetime column as the index.\n",
        "df = pd.DataFrame(spy_intra_data)\n",
        "df['day'] = pd.to_datetime(df['caldt']).dt.date  # Extract the date part from the datetime for daily analysis.\n",
        "df.set_index('caldt', inplace=True)  # Setting the datetime as the index for easier time series manipulation.\n",
        "\n",
        "# Group the DataFrame by the 'day' column to facilitate operations that need daily aggregation.\n",
        "daily_groups = df.groupby('day')\n",
        "\n",
        "# Extract unique days from the dataset to iterate through each day for processing.\n",
        "all_days = df['day'].unique()\n",
        "\n",
        "# Initialize new columns to store calculated metrics, starting with NaN for absence of initial values.\n",
        "df['move_open'] = np.nan  # To record the absolute daily change from the open price\n",
        "df['vwap'] = np.nan       # To calculate the Volume Weighted Average Price.\n",
        "df['spy_dvol'] = np.nan   # To record SPY's daily volatility.\n",
        "\n",
        "# Create a series to hold computed daily returns for SPY, initialized with NaN.\n",
        "spy_ret = pd.Series(index=all_days, dtype=float)\n",
        "\n",
        "# Iterate through each day to calculate metrics.\n",
        "for d in range(1, len(all_days)):\n",
        "    current_day = all_days[d]\n",
        "    prev_day = all_days[d - 1]\n",
        "\n",
        "    # Access the data for the current and previous days using their groups.\n",
        "    current_day_data = daily_groups.get_group(current_day)\n",
        "    prev_day_data = daily_groups.get_group(prev_day)\n",
        "\n",
        "    # Calculate the average of high, low, and close prices.\n",
        "    hlc = (current_day_data['high'] + current_day_data['low'] + current_day_data['close']) / 3\n",
        "\n",
        "    # Compute volume-weighted metrics for VWAP calculation.\n",
        "    vol_x_hlc = current_day_data['volume'] * hlc\n",
        "    cum_vol_x_hlc = vol_x_hlc.cumsum()  # Cumulative sum for VWAP calculation.\n",
        "    cum_volume = current_day_data['volume'].cumsum()\n",
        "\n",
        "    # Assign the calculated VWAP to the corresponding index in the DataFrame.\n",
        "    df.loc[current_day_data.index, 'vwap'] = cum_vol_x_hlc / cum_volume\n",
        "\n",
        "    # Calculate the absolute percentage change from the day's opening price.\n",
        "    open_price = current_day_data['open'].iloc[0]\n",
        "    df.loc[current_day_data.index, 'move_open'] = (current_day_data['close'] / open_price - 1).abs()\n",
        "\n",
        "    # Compute the daily return for SPY using the closing prices from the current and previous day.\n",
        "    spy_ret.loc[current_day] = current_day_data['close'].iloc[-1] / prev_day_data['close'].iloc[-1] - 1\n",
        "\n",
        "    # Calculate the 15-day rolling volatility, starting calculation after accumulating 15 days of data.\n",
        "    if d > 14:\n",
        "        df.loc[current_day_data.index, 'spy_dvol'] = spy_ret.iloc[d - 15:d - 1].std(skipna=False)\n",
        "\n",
        "# Calculate the minutes from market open and determine the minute of the day for each timestamp.\n",
        "df['min_from_open'] = ((df.index - df.index.normalize()) / pd.Timedelta(minutes=1)) - (9 * 60 + 30) + 1\n",
        "df['minute_of_day'] = df['min_from_open'].round().astype(int)\n",
        "\n",
        "# Group data by 'minute_of_day' for minute-level calculations.\n",
        "minute_groups = df.groupby('minute_of_day')\n",
        "\n",
        "# Calculate rolling mean and delayed sigma for each minute of the trading day.\n",
        "df['move_open_rolling_mean'] = minute_groups['move_open'].transform(lambda x: x.rolling(window=14, min_periods=13).mean())\n",
        "df['sigma_open'] = minute_groups['move_open_rolling_mean'].transform(lambda x: x.shift(1))\n",
        "\n",
        "# Convert dividend dates to datetime and merge dividend data based on trading days.\n",
        "dividends['day'] = pd.to_datetime(dividends['caldt']).dt.date\n",
        "df = df.merge(dividends[['day', 'dividend']], on='day', how='left')\n",
        "df['dividend'] = df['dividend'].fillna(0)  # Fill missing dividend data with 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeIDWZ-qQ9LY"
      },
      "source": [
        "# 3. Backtest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dj37jJM1Q9LY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Constants and settings\n",
        "AUM_0 = 100000.0\n",
        "commission = 0.0035\n",
        "min_comm_per_order = 0.35\n",
        "band_mult = 1\n",
        "band_simplified = 0\n",
        "trade_freq = 30\n",
        "sizing_type = \"vol_target\"\n",
        "target_vol = 0.02\n",
        "max_leverage = 4\n",
        "\n",
        "\n",
        "# Group data by day for faster access\n",
        "daily_groups = df.groupby('day')\n",
        "\n",
        "# Initialize strategy DataFrame using unique days\n",
        "strat = pd.DataFrame(index=all_days)\n",
        "strat['ret'] = np.nan\n",
        "strat['AUM'] = AUM_0\n",
        "strat['ret_spy'] = np.nan\n",
        "\n",
        "# Calculate daily returns for SPY using the closing prices\n",
        "df_daily = pd.DataFrame(spy_daily_data)\n",
        "df_daily['caldt'] = pd.to_datetime(df_daily['caldt']).dt.date\n",
        "df_daily.set_index('caldt', inplace=True)  # Set the datetime column as the DataFrame index for easy time series manipulation.\n",
        "\n",
        "df_daily['ret'] = df_daily['close'].diff() / df_daily['close'].shift()\n",
        "\n",
        "\n",
        "# Loop through all days, starting from the second day\n",
        "for d in range(1, len(all_days)):\n",
        "    current_day = all_days[d]\n",
        "    prev_day = all_days[d-1]\n",
        "\n",
        "    if prev_day in daily_groups.groups and current_day in daily_groups.groups:\n",
        "        prev_day_data = daily_groups.get_group(prev_day)\n",
        "        current_day_data = daily_groups.get_group(current_day)\n",
        "\n",
        "        if 'sigma_open' in current_day_data.columns and current_day_data['sigma_open'].isna().all():\n",
        "            continue\n",
        "\n",
        "        prev_close_adjusted = prev_day_data['close'].iloc[-1] - df.loc[current_day_data.index, 'dividend'].iloc[-1]\n",
        "\n",
        "        open_price = current_day_data['open'].iloc[0]\n",
        "        current_close_prices = current_day_data['close']\n",
        "        spx_vol = current_day_data['spy_dvol'].iloc[0]\n",
        "        vwap = current_day_data['vwap']\n",
        "\n",
        "        sigma_open = current_day_data['sigma_open']\n",
        "        UB = max(open_price, prev_close_adjusted) * (1 + band_mult * sigma_open)\n",
        "        LB = min(open_price, prev_close_adjusted) * (1 - band_mult * sigma_open)\n",
        "\n",
        "        # Determine trading signals\n",
        "        signals = np.zeros_like(current_close_prices)\n",
        "        signals[(current_close_prices > UB) & (current_close_prices > vwap)] = 1\n",
        "        signals[(current_close_prices < LB) & (current_close_prices < vwap)] = -1\n",
        "\n",
        "\n",
        "        # Position sizing\n",
        "        previous_aum = strat.loc[prev_day, 'AUM']\n",
        "\n",
        "        if sizing_type == \"vol_target\":\n",
        "            if math.isnan(spx_vol):\n",
        "                shares = round(previous_aum / open_price * max_leverage)\n",
        "            else:\n",
        "                shares = round(previous_aum / open_price * min(target_vol / spx_vol, max_leverage))\n",
        "\n",
        "        elif sizing_type == \"full_notional\":\n",
        "            shares = round(previous_aum / open_price)\n",
        "\n",
        "        # Apply trading signals at trade frequencies\n",
        "        trade_indices = np.where(current_day_data[\"min_from_open\"] % trade_freq == 0)[0]\n",
        "        exposure = np.full(len(current_day_data), np.nan)  # Start with NaNs\n",
        "        exposure[trade_indices] = signals[trade_indices]  # Apply signals at trade times\n",
        "\n",
        "        # Custom forward-fill that stops at zeros\n",
        "        last_valid = np.nan  # Initialize last valid value as NaN\n",
        "        filled_values = []   # List to hold the forward-filled values\n",
        "        for value in exposure:\n",
        "            if not np.isnan(value):  # If current value is not NaN, update last valid value\n",
        "                last_valid = value\n",
        "            if last_valid == 0:  # Reset if last valid value is zero\n",
        "                last_valid = np.nan\n",
        "            filled_values.append(last_valid)\n",
        "\n",
        "        exposure = pd.Series(filled_values, index=current_day_data.index).shift(1).fillna(0).values  # Apply shift and fill NaNs\n",
        "\n",
        "        # Calculate trades count based on changes in exposure\n",
        "        trades_count = np.sum(np.abs(np.diff(np.append(exposure, 0))))\n",
        "\n",
        "        # Calculate PnL\n",
        "        change_1m = current_close_prices.diff()\n",
        "        gross_pnl = np.sum(exposure * change_1m) * shares\n",
        "        commission_paid = trades_count * max(min_comm_per_order, commission * shares)\n",
        "        net_pnl = gross_pnl - commission_paid\n",
        "\n",
        "        # Update the daily return and new AUM\n",
        "        strat.loc[current_day, 'AUM'] = previous_aum + net_pnl\n",
        "        strat.loc[current_day, 'ret'] = net_pnl / previous_aum\n",
        "\n",
        "        # Save the passive Buy&Hold daily return for SPY\n",
        "        strat.loc[current_day, 'ret_spy'] = df_daily.loc[df_daily.index == current_day, 'ret'].values[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXGOeFFiQ9LZ"
      },
      "source": [
        "# 4. Study Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgQcpbIPQ9LZ"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative products for AUM calculations\n",
        "strat['AUM_SPX'] = AUM_0 * (1 + strat['ret_spy']).cumprod(skipna=True)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plotting the AUM of the strategy and the passive S&P 500 exposure\n",
        "ax.plot(strat.index, strat['AUM'], label='Momentum', linewidth=2, color='k')\n",
        "ax.plot(strat.index, strat['AUM_SPX'], label=f'{ticker} Buy & Hold', linewidth=1, color='r')\n",
        "\n",
        "# Formatting the plot\n",
        "ax.grid(True, linestyle=':')\n",
        "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y'))\n",
        "plt.xticks(rotation=90)\n",
        "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, _: f'${x:,.0f}'))\n",
        "ax.set_ylabel('AUM ($)')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Intraday Momentum Strategy', fontsize=12, fontweight='bold')\n",
        "plt.suptitle(f'Commission = ${commission}/share', fontsize=9, verticalalignment='top')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate additional stats and display them\n",
        "stats = {\n",
        "    'Total Return (%)': round((np.prod(1 + strat['ret'].dropna()) - 1) * 100, 0),\n",
        "    'Annualized Return (%)': round((np.prod(1 + strat['ret']) ** (252 / len(strat['ret'])) - 1) * 100, 1),\n",
        "    'Annualized Volatility (%)': round(strat['ret'].dropna().std() * np.sqrt(252) * 100, 1),\n",
        "    'Sharpe Ratio': round(strat['ret'].dropna().mean() / strat['ret'].dropna().std() * np.sqrt(252), 2),\n",
        "    'Hit Ratio (%)': round((strat['ret'] > 0).sum() / (strat['ret'].abs() > 0).sum() * 100, 0),\n",
        "    'Maximum Drawdown (%)': round(strat['AUM'].div(strat['AUM'].cummax()).sub(1).min() * -100, 0)\n",
        "}\n",
        "\n",
        "\n",
        "Y = strat['ret'].dropna()\n",
        "X = sm.add_constant(strat['ret_spy'].dropna())\n",
        "model = sm.OLS(Y, X).fit()\n",
        "stats['Alpha (%)'] = round(model.params.const * 100 * 252, 2)\n",
        "stats['Beta'] = round(model.params['ret_spy'], 2)\n",
        "\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pSuhuqzQ9Lb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}